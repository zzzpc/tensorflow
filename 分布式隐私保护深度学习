# coding=utf-8
import numpy as np
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import  negative
import floatTbin
import getLabels
import  time
from keras.utils import to_categorical
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

train_x,train_y,test_x,test_y=getLabels.run()
train_y=to_categorical(train_y)
test_y=to_categorical(test_y)
class Dataset:
    def __init__(self,data,label):
        self._index_in_epoch = 0
        self._epochs_completed = 0
        self._data = data
        self._label = label
        self._num_examples = data.shape[0]
        pass
    @property
    def data(self):
        return self._data

    @property
    def label(self):
        return self._label

    def next_batch(self,batch_size,shuffle = True):
        start = self._index_in_epoch
        if start == 0 and self._epochs_completed == 0:
            idx = np.arange(0, self._num_examples)  # get all possible indexes

            np.random.shuffle(idx)  # shuffle indexe 打乱顺序

            self._data = (self.data[idx]) # get list of `num` random samples
            self._label = self.label[idx]

        # go to the next batch
        if start + batch_size > self._num_examples:
            self._epochs_completed += 1
            rest_num_examples = self._num_examples - start
            data_rest_part = self.data[start:self._num_examples]
            label_rest_part = self.label[start:self._num_examples]

            idx0 = np.arange(0, self._num_examples)  # get all possible indexes
            np.random.shuffle(idx0)  # shuffle indexes
            self._data = self.data[idx0]  # get list of `num` random samples
            self._label = self.label[idx0]  # get list of `num` random samples

            start = 0
            self._index_in_epoch = batch_size - rest_num_examples #avoid the case where the #sample != integar times of batch_size
            end =  self._index_in_epoch
            data_new_part =  self._data[start:end]
            label_new_part =  self._label[start:end]
            return np.concatenate((data_rest_part, data_new_part), axis=0), np.concatenate((label_rest_part, label_new_part), axis=0)
        else:
            self._index_in_epoch += batch_size
            end = self._index_in_epoch
            return self._data[start:end], self._label[start:end]

num_client=2

client_1=Dataset(train_x[:30000],train_y[:30000])

client_2=Dataset(train_x[30000:],train_y[30000:])


# Define parameters
#  python distributed.py --ps_hosts=127.0.0.1:2222 --worker_hosts=127.0.0.1:2225,127.0.0.1:2226 --job_name=ps --task_index=0
#  python distributed.py --ps_hosts=127.0.0.1:2222 --worker_hosts=127.0.0.1:2225,127.0.0.1:2226 --job_name=worker --task_index=0
#  python distributed.py --ps_hosts=127.0.0.1:2222 --worker_hosts=127.0.0.1:2225,127.0.0.1:2226 --job_name=worker --task_index=1
FLAGS = tf.app.flags.FLAGS
tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')
tf.app.flags.DEFINE_integer('steps_to_validate', 3,
                            'Steps to validate and print loss')

# For distributed
tf.app.flags.DEFINE_string("ps_hosts", "",
                           "Comma-separated list of hostname:port pairs")
tf.app.flags.DEFINE_string("worker_hosts", "",
                           "Comma-separated list of hostname:port pairs")
tf.app.flags.DEFINE_string("job_name", "", "One of 'ps', 'worker'")
tf.app.flags.DEFINE_integer("task_index", 0, "Index of task within the job")
tf.app.flags.DEFINE_integer("issync", 0, "是否采用分布式的同步模式，1表示同步模式，0表示异步模式")

# Hyperparameters
learning_rate = FLAGS.learning_rate
steps_to_validate = FLAGS.steps_to_validate

# 定义初始化权值函数
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)


# 定义初始化偏置函数
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)


# 卷积层
def conv2d(input, filter):
    return tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')


# 池化层
def max_pool_2x2(value):
    return tf.nn.max_pool(value, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')


def Lenet(x,y_,keep_prob):

    x_image = tf.reshape(x, [-1, 28, 28, 1])

    W_conv1 = weight_variable([3, 3, 1, 32])
    b_conv1 = bias_variable([32])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)

    W_conv2 = weight_variable([3, 3, 32, 64])
    b_conv2 = bias_variable([64])
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool_2x2(h_conv2)

    W_fc1 = weight_variable([7 * 7 * 64, 100])
    b_fc1 = bias_variable([100])
    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)


    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

    W_fc2 = weight_variable([100, 10])
    b_fc2 = bias_variable([10])
    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)

    return y_conv


def main(_):
    ps_hosts = FLAGS.ps_hosts.split(",")
    worker_hosts = FLAGS.worker_hosts.split(",")
    cluster = tf.train.ClusterSpec({"ps": ps_hosts, "worker": worker_hosts})
    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)

    issync = FLAGS.issync
    if FLAGS.job_name == "ps":
        server.join()
    elif FLAGS.job_name == "worker":
        with tf.device(tf.train.replica_device_setter(
                worker_device="/job:worker/task:%d" % FLAGS.task_index,
                cluster=cluster)):
            global_step = tf.Variable(0, name='global_step', trainable=False)

            x = tf.placeholder(tf.float32, [None, 784])
            y_ = tf.placeholder(tf.float32, [None, 10])
            keep_prob = tf.placeholder(tf.float32)
            output=Lenet(x,y_,keep_prob)

            loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(output), reduction_indices=[1]))
            optimizer = tf.train.GradientDescentOptimizer(learning_rate)
            grads_and_vars = optimizer.compute_gradients(loss)
            train_step=optimizer.minimize(loss)
            var_1 = [tf.trainable_variables()[0]]
            var_2 = [tf.trainable_variables()[2]]
            var_3 = [tf.trainable_variables()[4]]
            var_4 = [tf.trainable_variables()[6]]
            var_5 = [tf.trainable_variables()[1]]
            var_6 = [tf.trainable_variables()[3]]
            var_7 = [tf.trainable_variables()[5]]
            var_8 = [tf.trainable_variables()[7]]


            if issync == 1:
                # 同步模式计算更新梯度
                rep_op = tf.train.SyncReplicasOptimizer(optimizer,
                                                        replicas_to_aggregate=len(
                                                            worker_hosts),
                                                        #replica_id=FLAGS.task_index,
                                                        total_num_replicas=len(
                                                            worker_hosts),
                                                        use_locking=False)
                train_op = rep_op.apply_gradients(grads_and_vars,
                                                  global_step=global_step)
                init_token_op = rep_op.get_init_tokens_op()
                chief_queue_runner = rep_op.get_chief_queue_runner()
            else:
                # 异步模式计算更新梯度
                train_op = optimizer.apply_gradients(grads_and_vars,
                                                     global_step=global_step)

            init_op = tf.initialize_all_variables()

            saver = tf.train.Saver()
            #tf.summary.scalar('cost', loss_value)
            #summary_op = tf.summary.merge_all()

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),
                                 logdir="./checkpoint/",
                                 init_op=init_op,
                                 summary_op=None,
                                 saver=saver,
                                 global_step=global_step,
                                 save_model_secs=60)

        with sv.prepare_or_wait_for_session(server.target) as sess:
            # 如果是同步模式
            if FLAGS.task_index == 0 and issync == 1:
                sv.start_queue_runners(sess, [chief_queue_runner])
                sess.run(init_token_op)
            step = 0
            while step < 100:
                if FLAGS.task_index == 0:
                    batch=client_1.next_batch(32)
                    print('我是用户1')
                else:
                    batch = client_2.next_batch(32)
                    print('我是用户2')

                weight = []
                recover_weight = []
                s = ""
                gradient_ori = sess.run([grads_and_vars], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
                print('新一轮的权重参数')
                print(sess.run(tf.trainable_variables()[0]))
                time1 = time.time()
                for i in range(8):
                    flat = list(np.array(gradient_ori[0][i][0]).flatten())
                    for j in range(len(flat)):
                        weight.append(flat[j])
                print('原始梯度', weight[:10])
                remainder = len(weight) % 1000
                print(remainder, len(weight))
                for i in range(len(weight[:-remainder])):
                    s += floatTbin.conFra(weight[i])
                    if (i + 1) % 200 == 0:
                        n_weight = negative.main(s)
                        for each in n_weight:
                            recover_weight.append(each)
                        s = ''
                for i in range(len(weight[-remainder:])):
                    s += floatTbin.conFra(weight[i])
                n_weight = negative.main(s)
                for each in n_weight:
                    recover_weight.append(each)
                print('重构梯度：', recover_weight[:10])
                time2 = time.time()
                print('重构时间：', (time2 - time1))
                sess.graph._unsafe_unfinalize()
                grads_and_vars1 = list(zip([tf.convert_to_tensor(np.array(recover_weight[:288]).reshape(3, 3, 1, 32), dtype=tf.float32)],var_1))
                grads_and_vars5 = list(zip([tf.convert_to_tensor(np.array(recover_weight[288:320]).reshape(32), dtype=tf.float32)], var_5))
                grads_and_vars2 = list(zip([tf.convert_to_tensor(np.array(recover_weight[320:18752]).reshape(3, 3, 32, 64), dtype=tf.float32)],var_2))
                grads_and_vars6 = list(zip([tf.convert_to_tensor(np.array(recover_weight[18752:18816]).reshape(64), dtype=tf.float32)],var_6))
                grads_and_vars3 = list(zip([tf.convert_to_tensor(np.array(recover_weight[18816:332416]).reshape(3136, 100), dtype=tf.float32)],var_3))
                grads_and_vars7 = list(zip([tf.convert_to_tensor(np.array(recover_weight[332416:332516]).reshape(100), dtype=tf.float32)],var_7))
                grads_and_vars4 = list(zip([tf.convert_to_tensor(np.array(recover_weight[332516:333516]).reshape(100, 10), dtype=tf.float32)],var_4))
                grads_and_vars8 = list(zip([tf.convert_to_tensor(np.array(recover_weight[333516:333526]).reshape(10), dtype=tf.float32)],var_8))
                grads_and_vars = grads_and_vars1 + grads_and_vars5 + grads_and_vars2 + grads_and_vars6 + grads_and_vars3 + grads_and_vars7 + grads_and_vars4 + grads_and_vars8

                _, step, gradient, loss_value = sess.run([train_op, global_step, grads_and_vars, loss], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})


                #_, step, gradient, loss_value = sess.run([train_step, global_step, grads_and_vars, loss],feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
                if step % steps_to_validate == 0:
                    print("step: %d, loss: %f" % (step, loss_value))
        sv.stop()




if __name__ == "__main__":
    tf.app.run()

















